commit 7303c911f7830783188377f366d83d9f174dc5da
Author: FuzzyQuils <fuzzyquils@scrapyard.link>
Date:   Sun Jan 15 20:48:56 2023 +1000

    (STAGING) Implement Low Fragmentation Heap from Remi Bernon
    
    Pls fix Overwatch 2...

diff --git a/dlls/msvcrt/heap.c b/dlls/msvcrt/heap.c
index bf06c37e2c5..d187fbe9b8c 100644
--- a/dlls/msvcrt/heap.c
+++ b/dlls/msvcrt/heap.c
@@ -530,7 +530,9 @@ int CDECL _set_sbh_threshold(size_t threshold)
 
   if(!sb_heap)
   {
+      ULONG hci = 2;
       sb_heap = HeapCreate(0, 0, 0);
+      HeapSetInformation(sb_heap, HeapCompatibilityInformation, &hci, sizeof(hci));
       if(!sb_heap)
           return 0;
   }
@@ -829,7 +831,9 @@ int CDECL wmemcpy_s(wchar_t *dest, size_t numberOfElements,
 
 BOOL msvcrt_init_heap(void)
 {
+    ULONG hci = 2;
     heap = HeapCreate(0, 0, 0);
+    HeapSetInformation(heap, HeapCompatibilityInformation, &hci, sizeof(hci));
     return heap != NULL;
 }
 
diff --git a/dlls/ntdll/Makefile.in b/dlls/ntdll/Makefile.in
index 6202ef142df..0206f24c3f7 100644
--- a/dlls/ntdll/Makefile.in
+++ b/dlls/ntdll/Makefile.in
@@ -20,6 +20,7 @@ C_SRCS = \
 	exception.c \
 	handletable.c \
 	heap.c \
+	heap_lfh.c \
 	large_int.c \
 	loader.c \
 	locale.c \
diff --git a/dlls/ntdll/heap.c b/dlls/ntdll/heap.c
index 055a2674433..bf373b80d25 100644
--- a/dlls/ntdll/heap.c
+++ b/dlls/ntdll/heap.c
@@ -39,6 +39,13 @@
 
 WINE_DEFAULT_DEBUG_CHANNEL(heap);
 
+/* HeapCompatibilityInformation values */
+
+#define HEAP_STD 0
+#define HEAP_LAL 1
+#define HEAP_LFH 2
+
+
 /* undocumented RtlWalkHeap structure */
 
 struct rtl_heap_entry
@@ -194,6 +201,7 @@ struct heap
     DWORD            force_flags;   /* 0044/0074 */
     /* end of the Windows 10 compatible struct layout */
 
+    volatile LONG    compat_info;   /* HeapCompatibilityInformation / heap frontend type */
     struct list      entry;         /* Entry in process heap list */
     struct list      subheap_list;  /* Sub-heap list */
     struct list      large_list;    /* Large blocks list */
@@ -217,15 +225,6 @@ C_ASSERT( offsetof(struct heap, subheap) <= REGION_ALIGN - 1 );
 #define HEAP_DEF_SIZE        (0x40000 * BLOCK_ALIGN)
 #define MAX_FREE_PENDING     1024    /* max number of free requests to delay */
 
-/* some undocumented flags (names are made up) */
-#define HEAP_PRIVATE          0x00001000
-#define HEAP_ADD_USER_INFO    0x00000100
-#define HEAP_USER_FLAGS_MASK  0x00000f00
-#define HEAP_PAGE_ALLOCS      0x01000000
-#define HEAP_VALIDATE         0x10000000
-#define HEAP_VALIDATE_ALL     0x20000000
-#define HEAP_VALIDATE_PARAMS  0x40000000
-#define HEAP_CHECKING_ENABLED 0x80000000
 
 static struct heap *process_heap;  /* main process heap */
 
@@ -1295,6 +1294,8 @@ static void heap_set_debug_flags( HANDLE handle )
                                               MAX_FREE_PENDING * sizeof(*heap->pending_free) );
         heap->pending_pos = 0;
     }
+
+    HEAP_lfh_set_debug_flags( flags );
 }
 
 
@@ -1343,6 +1344,7 @@ HANDLE WINAPI RtlCreateHeap( ULONG flags, void *addr, SIZE_T total_size, SIZE_T
     heap->ffeeffee      = 0xffeeffee;
     heap->auto_flags    = (flags & HEAP_GROWABLE);
     heap->flags         = (flags & ~HEAP_SHARED);
+    heap->compat_info   = HEAP_STD;
     heap->magic         = HEAP_MAGIC;
     heap->grow_size     = max( HEAP_DEF_SIZE, total_size );
     heap->min_size      = commit_size;
@@ -1541,6 +1543,8 @@ void *WINAPI DECLSPEC_HOTPATCH RtlAllocateHeap( HANDLE handle, ULONG flags, SIZE
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_INVALID_HANDLE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_allocate( heap, heap_flags, size, &ptr ))
+        status = STATUS_SUCCESS;
     else if ((block_size = heap_get_block_size( heap, heap_flags, size )) == ~0U)
         status = STATUS_NO_MEMORY;
     else if (block_size >= HEAP_MIN_LARGE_BLOCK_SIZE)
@@ -1576,6 +1580,8 @@ BOOLEAN WINAPI DECLSPEC_HOTPATCH RtlFreeHeap( HANDLE handle, ULONG flags, void *
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_INVALID_PARAMETER;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_free( heap, heap_flags, ptr ))
+        status = STATUS_SUCCESS;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
         status = STATUS_INVALID_PARAMETER;
     else if (block_get_flags( block ) & BLOCK_FLAG_LARGE)
@@ -1684,6 +1690,8 @@ void *WINAPI RtlReAllocateHeap( HANDLE handle, ULONG flags, void *ptr, SIZE_T si
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_INVALID_HANDLE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_reallocate( heap, heap_flags, ptr, size, &ret ))
+        status = STATUS_SUCCESS;
     else if ((block_size = heap_get_block_size( heap, heap_flags, size )) == ~0U)
         status = STATUS_NO_MEMORY;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
@@ -1801,6 +1809,8 @@ SIZE_T WINAPI RtlSizeHeap( HANDLE handle, ULONG flags, const void *ptr )
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_INVALID_PARAMETER;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_get_allocated_size( heap, heap_flags, ptr, &size ))
+        status = STATUS_SUCCESS;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
         status = STATUS_INVALID_PARAMETER;
     else
@@ -1827,6 +1837,8 @@ BOOLEAN WINAPI RtlValidateHeap( HANDLE handle, ULONG flags, const void *ptr )
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         ret = FALSE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_validate( heap, heap_flags, ptr ))
+        ret = TRUE;
     else
     {
         heap_lock( heap, heap_flags );
@@ -2004,21 +2016,25 @@ ULONG WINAPI RtlGetProcessHeaps( ULONG count, HANDLE *heaps )
  *           RtlQueryHeapInformation    (NTDLL.@)
  */
 NTSTATUS WINAPI RtlQueryHeapInformation( HANDLE handle, HEAP_INFORMATION_CLASS info_class,
-                                         void *info, SIZE_T size_in, PSIZE_T size_out )
+                                         void *info, SIZE_T size_in, SIZE_T *size_out )
 {
+    struct heap *heap;
+    ULONG heap_flags;
+
+    TRACE( "handle %p, info_class %u, info %p, size_in %Iu, size_out %p.\n", handle, info_class, info, size_in, size_out );
+
     switch (info_class)
     {
     case HeapCompatibilityInformation:
+        if (!(heap = unsafe_heap_from_handle( handle, 0, &heap_flags ))) return STATUS_ACCESS_VIOLATION;
         if (size_out) *size_out = sizeof(ULONG);
 
-        if (size_in < sizeof(ULONG))
-            return STATUS_BUFFER_TOO_SMALL;
-
-        *(ULONG *)info = 0; /* standard heap */
+        if (size_in < sizeof(ULONG)) return STATUS_BUFFER_TOO_SMALL;
+        *(ULONG *)info = heap->compat_info;
         return STATUS_SUCCESS;
 
     default:
-        FIXME("Unknown heap information class %u\n", info_class);
+        FIXME( "HEAP_INFORMATION_CLASS %u not implemented!\n", info_class );
         return STATUS_INVALID_INFO_CLASS;
     }
 }
@@ -2028,8 +2044,34 @@ NTSTATUS WINAPI RtlQueryHeapInformation( HANDLE handle, HEAP_INFORMATION_CLASS i
  */
 NTSTATUS WINAPI RtlSetHeapInformation( HANDLE handle, HEAP_INFORMATION_CLASS info_class, void *info, SIZE_T size )
 {
-    FIXME( "handle %p, info_class %d, info %p, size %Id stub!\n", handle, info_class, info, size );
-    return STATUS_SUCCESS;
+    struct heap *heap;
+    ULONG heap_flags;
+
+    TRACE( "handle %p, info_class %u, info %p, size %Iu.\n", handle, info_class, info, size );
+
+    switch (info_class)
+    {
+    case HeapCompatibilityInformation:
+    {
+        ULONG compat_info;
+
+        if (size < sizeof(ULONG)) return STATUS_BUFFER_TOO_SMALL;
+        if (!(heap = unsafe_heap_from_handle( handle, 0, &heap_flags ))) return STATUS_INVALID_HANDLE;
+
+        compat_info = *(ULONG *)info;
+        if (compat_info != HEAP_STD && compat_info != HEAP_LFH)
+        {
+            FIXME( "HeapCompatibilityInformation %lu not implemented!\n", compat_info );
+            return STATUS_UNSUCCESSFUL;
+        }
+        if (InterlockedCompareExchange( &heap->compat_info, compat_info, HEAP_STD )) return STATUS_UNSUCCESSFUL;
+        return STATUS_SUCCESS;
+    }
+
+    default:
+        FIXME( "HEAP_INFORMATION_CLASS %u not implemented!\n", info_class );
+        return STATUS_SUCCESS;
+    }
 }
 
 /***********************************************************************
@@ -2050,6 +2092,8 @@ BOOLEAN WINAPI RtlGetUserInfoHeap( HANDLE handle, ULONG flags, void *ptr, void *
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_SUCCESS;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_get_user_info( heap, heap_flags, ptr, user_value, user_flags ))
+        status = STATUS_SUCCESS;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
     {
         status = STATUS_INVALID_PARAMETER;
@@ -2094,6 +2138,8 @@ BOOLEAN WINAPI RtlSetUserValueHeap( HANDLE handle, ULONG flags, void *ptr, void
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         ret = TRUE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_set_user_value( heap, heap_flags, ptr, user_value ))
+        ret = TRUE;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
         ret = FALSE;
     else if (!(block_get_flags( block ) & BLOCK_FLAG_USER_INFO))
@@ -2139,6 +2185,8 @@ BOOLEAN WINAPI RtlSetUserFlagsHeap( HANDLE handle, ULONG flags, void *ptr, ULONG
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         ret = TRUE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_set_user_flags( heap, heap_flags, ptr, clear, set ))
+        ret = TRUE;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
         ret = FALSE;
     else if (!(block_get_flags( block ) & BLOCK_FLAG_USER_INFO))
@@ -2151,3 +2199,8 @@ BOOLEAN WINAPI RtlSetUserFlagsHeap( HANDLE handle, ULONG flags, void *ptr, ULONG
 
     return ret;
 }
+
+void HEAP_notify_thread_destroy( BOOLEAN last )
+{
+    HEAP_lfh_notify_thread_destroy( last );
+}
\ No newline at end of file
diff --git a/dlls/ntdll/heap_lfh.c b/dlls/ntdll/heap_lfh.c
new file mode 100644
index 00000000000..f05c2e6ebcf
--- /dev/null
+++ b/dlls/ntdll/heap_lfh.c
@@ -0,0 +1,1236 @@
+/*
+ * Wine Low Fragmentation Heap
+ *
+ * Copyright 2020 Remi Bernon for CodeWeavers
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+
+#include "ntstatus.h"
+#define WIN32_NO_STATUS
+
+#include "wine/list.h"
+#include "wine/debug.h"
+
+#include "ntdll_misc.h"
+
+#if defined(__GNUC__) || defined(__clang__)
+#define likely(x) __builtin_expect(!!(x), 1)
+#define unlikely(x) __builtin_expect(!!(x), 0)
+#else
+#define likely(x) x
+#define unlikely(x) x
+#endif
+
+WINE_DEFAULT_DEBUG_CHANNEL(heap);
+
+#define ROUND_SIZE(size, mask) ((((SIZE_T)(size) + (mask)) & ~(SIZE_T)(mask)))
+#define ALIGNMENT (2 * sizeof(void *))
+
+#define BLOCK_FLAG_USER_INFO   0x00000010 /* user flags up to 0xf0 */
+#define BLOCK_FLAG_USER_MASK   0x000000f0
+
+#define BLOCK_USER_FLAGS( heap_flags ) (((heap_flags) >> 4) & BLOCK_FLAG_USER_MASK)
+#define HEAP_USER_FLAGS( block_flags ) (((block_flags) & BLOCK_FLAG_USER_MASK) << 4)
+
+typedef struct LFH_ptr LFH_ptr;
+typedef struct LFH_block LFH_block;
+typedef enum LFH_block_type LFH_block_type;
+typedef struct LFH_arena LFH_arena;
+typedef struct LFH_class LFH_class;
+typedef struct LFH_heap LFH_heap;
+typedef struct LFH_slist LFH_slist;
+
+#define ARENA_HEADER_SIZE (sizeof(LFH_arena))
+
+#define LARGE_ARENA_SIZE 0x400000 /* 4MiB */
+#define LARGE_ARENA_MASK (LARGE_ARENA_SIZE - 1)
+
+#define BLOCK_ARENA_SIZE 0x10000 /* 64kiB */
+#define BLOCK_ARENA_MASK (BLOCK_ARENA_SIZE - 1)
+
+#define SMALL_CLASS_STEP     0x20
+#define SMALL_CLASS_MASK     (SMALL_CLASS_STEP - 1)
+#define SMALL_CLASS_MIN_SIZE SMALL_CLASS_STEP
+#define SMALL_CLASS_MAX_SIZE 0x800
+#define SMALL_CLASS_COUNT    ((SMALL_CLASS_MAX_SIZE - SMALL_CLASS_MIN_SIZE) / SMALL_CLASS_STEP + 1)
+#define SMALL_CLASS_FIRST    0
+#define SMALL_CLASS_LAST     (SMALL_CLASS_FIRST + SMALL_CLASS_COUNT - 1)
+
+#define MEDIUM_CLASS_STEP     (16 * SMALL_CLASS_STEP)
+#define MEDIUM_CLASS_MASK     (MEDIUM_CLASS_STEP - 1)
+#define MEDIUM_CLASS_MIN_SIZE SMALL_CLASS_MAX_SIZE
+#define MEDIUM_CLASS_MAX_SIZE ((BLOCK_ARENA_SIZE - ARENA_HEADER_SIZE - ARENA_HEADER_SIZE) / 2)
+#define MEDIUM_CLASS_COUNT    ((MEDIUM_CLASS_MAX_SIZE - MEDIUM_CLASS_MIN_SIZE + MEDIUM_CLASS_MASK) / MEDIUM_CLASS_STEP + 1)
+#define MEDIUM_CLASS_FIRST    (SMALL_CLASS_LAST + 1)
+#define MEDIUM_CLASS_LAST     (MEDIUM_CLASS_FIRST + MEDIUM_CLASS_COUNT - 1)
+
+#define LARGE_CLASS_STEP      BLOCK_ARENA_SIZE
+#define LARGE_CLASS_MASK      (LARGE_CLASS_STEP - 1)
+#define LARGE_CLASS_MIN_SIZE  (BLOCK_ARENA_SIZE - ARENA_HEADER_SIZE)
+#define LARGE_CLASS_MAX_SIZE  (LARGE_ARENA_SIZE / 2 - ARENA_HEADER_SIZE) /* we need an arena header for every large block */
+#define LARGE_CLASS_COUNT     ((LARGE_CLASS_MAX_SIZE - LARGE_CLASS_MIN_SIZE) / LARGE_CLASS_STEP + 1)
+#define LARGE_CLASS_FIRST     0
+#define LARGE_CLASS_LAST      (LARGE_CLASS_FIRST + LARGE_CLASS_COUNT - 1)
+
+#define TOTAL_BLOCK_CLASS_COUNT (MEDIUM_CLASS_LAST + 1)
+#define TOTAL_LARGE_CLASS_COUNT (LARGE_CLASS_LAST + 1)
+
+struct LFH_slist
+{
+    LFH_slist *next;
+};
+
+static inline void LFH_slist_push(LFH_slist **list, LFH_slist *entry)
+{
+    /* There will be no ABA issue here, other threads can only replace
+     * list->next with a different entry, or NULL. */
+    entry->next = __atomic_load_n(list, __ATOMIC_RELAXED);
+    while (!__atomic_compare_exchange_n(list, &entry->next, entry, 0, __ATOMIC_RELEASE, __ATOMIC_ACQUIRE));
+}
+
+static inline LFH_slist *LFH_slist_flush(LFH_slist **list)
+{
+    if (!__atomic_load_n(list, __ATOMIC_RELAXED)) return NULL;
+    return __atomic_exchange_n(list, NULL, __ATOMIC_ACQUIRE);
+}
+
+/* be sure to keep these different from ARENA_INUSE magic */
+enum LFH_block_type
+{
+    LFH_block_type_used = 0xa55a5aa5,
+    LFH_block_type_free = 0xc33c3cc3,
+};
+
+struct DECLSPEC_ALIGN(16) LFH_block
+{
+    union
+    {
+        ssize_t next_free;
+        LFH_slist entry_defer;
+        size_t alloc_size;
+    };
+
+    UINT32 flags;
+    LFH_block_type type;
+};
+
+C_ASSERT(sizeof(LFH_block) == 0x10);
+C_ASSERT(offsetof(LFH_block, entry_defer) == 0);
+
+struct DECLSPEC_ALIGN(16) LFH_arena
+{
+    ssize_t next_free;
+    LFH_arena *class_entry;
+
+    union
+    {
+        LFH_arena *parent;
+        LFH_class *class;
+    };
+
+    union
+    {
+        size_t huge_size;
+        size_t used_count;
+    };
+};
+
+#ifdef _WIN64
+C_ASSERT(sizeof(LFH_arena) == 0x20);
+#else
+C_ASSERT(sizeof(LFH_arena) == 0x10);
+#endif
+
+struct LFH_class
+{
+    LFH_arena *next;
+    size_t     size;
+};
+
+struct LFH_heap
+{
+    LFH_slist *list_defer;
+    LFH_arena *cached_large_arena;
+
+    LFH_class block_class[TOTAL_BLOCK_CLASS_COUNT];
+    LFH_class large_class[TOTAL_LARGE_CLASS_COUNT];
+
+    SLIST_ENTRY entry_orphan;
+#ifdef _WIN64
+    void *pad[0xc2];
+#else
+    void *pad[0xc3];
+#endif
+};
+
+C_ASSERT(TOTAL_BLOCK_CLASS_COUNT == 0x7d);
+C_ASSERT(TOTAL_LARGE_CLASS_COUNT == 0x20);
+
+/* arena->class/arena->parent pointer low bits are used to discriminate between the two */
+C_ASSERT(offsetof(LFH_heap, block_class[0]) > 0);
+C_ASSERT(offsetof(LFH_heap, large_class[TOTAL_LARGE_CLASS_COUNT]) < BLOCK_ARENA_SIZE);
+
+/* helpers to retrieve parent arena from a child, or class pointer from a large or block arena */
+static inline LFH_arena *LFH_parent_from_arena(const LFH_arena *arena)
+{
+    if (!arena->parent) return (LFH_arena *)arena;
+    if (!((UINT_PTR)(arena)->parent & BLOCK_ARENA_MASK)) return arena->parent;
+    return (LFH_arena *)arena;
+}
+
+static inline LFH_class *LFH_class_from_arena(const LFH_arena *arena)
+{
+    const LFH_arena *parent = LFH_parent_from_arena(arena);
+    if ((UINT_PTR)parent->class & BLOCK_ARENA_MASK) return parent->class;
+    return NULL;
+}
+
+/* make sure its aligns to power of two so we can mask class pointers in LFH_heap_from_arena */
+#ifdef _WIN64
+C_ASSERT(sizeof(LFH_heap) == 0x1000);
+#else
+C_ASSERT(sizeof(LFH_heap) == 0x800);
+#endif
+
+/* helper to retrieve the heap from an arena, using its class pointer */
+static inline LFH_heap *LFH_heap_from_arena(const LFH_arena *arena)
+{
+    LFH_class *class = LFH_class_from_arena(arena);
+    return (LFH_heap *)((UINT_PTR)class & ~(sizeof(LFH_heap) - 1));
+}
+
+/* helpers to retrieve block pointers to the containing block or large (maybe child) arena */
+static inline LFH_arena *LFH_large_arena_from_block(const LFH_block *block)
+{
+    return (LFH_arena *)((UINT_PTR)block & ~BLOCK_ARENA_MASK);
+}
+
+static inline LFH_arena *LFH_block_arena_from_block(const LFH_block *block)
+{
+    return LFH_large_arena_from_block(block) + 1;
+}
+
+static inline LFH_arena *LFH_arena_from_block(const LFH_block *block)
+{
+    LFH_arena *block_arena = LFH_block_arena_from_block(block);
+    if (block_arena == (LFH_arena *)block) return LFH_large_arena_from_block(block);
+    return block_arena;
+}
+
+/* helpers to translate between data pointer and LFH_block header */
+static inline LFH_block *LFH_block_from_ptr(const LFH_ptr *ptr)
+{
+    return ((LFH_block *)ptr) - 1;
+}
+
+static inline void *LFH_ptr_from_block(const LFH_block *block)
+{
+    return (LFH_ptr *)(block + 1);
+}
+
+static inline size_t LFH_block_get_class_size(const LFH_block *block)
+{
+    const LFH_arena *arena = LFH_arena_from_block(block);
+    const LFH_class *class = LFH_class_from_arena(arena);
+    if (class) return class->size;
+    return arena->huge_size;
+}
+
+static inline size_t LFH_block_get_alloc_size(const LFH_block *block, ULONG flags)
+{
+    return block->alloc_size;
+}
+
+static inline size_t LFH_get_class_size(ULONG flags, size_t size)
+{
+    static const ULONG padd_flags = HEAP_VALIDATE | HEAP_VALIDATE_ALL | HEAP_VALIDATE_PARAMS | HEAP_ADD_USER_INFO;
+    static const ULONG check_flags = HEAP_TAIL_CHECKING_ENABLED | HEAP_FREE_CHECKING_ENABLED | HEAP_CHECKING_ENABLED;
+    SIZE_T overhead;
+
+    if ((flags & check_flags)) overhead = ALIGNMENT;
+    else overhead = sizeof(LFH_block);
+
+    if (flags & HEAP_TAIL_CHECKING_ENABLED) overhead += ALIGNMENT;
+    if (flags & padd_flags) overhead += ALIGNMENT;
+
+    if (size < ALIGNMENT) size = ALIGNMENT;
+    return ROUND_SIZE( size + overhead, ALIGNMENT - 1 );
+}
+
+static inline void *LFH_memory_allocate(size_t size)
+{
+    void *addr = NULL;
+    SIZE_T alloc_size = size;
+
+    if (NtAllocateVirtualMemory(NtCurrentProcess(), (void **)&addr, 0, &alloc_size,
+                                MEM_RESERVE | MEM_COMMIT, PAGE_READWRITE))
+        return NULL;
+
+    return addr;
+}
+
+static inline BOOLEAN LFH_memory_deallocate(void *addr, size_t size)
+{
+    SIZE_T release_size = 0;
+
+    if (NtFreeVirtualMemory(NtCurrentProcess(), &addr, &release_size, MEM_RELEASE))
+        return FALSE;
+
+    return TRUE;
+}
+
+static inline LFH_block *LFH_arena_get_block(const LFH_arena *arena, size_t offset)
+{
+    return (LFH_block *)((UINT_PTR)arena + offset);
+}
+
+static inline void LFH_arena_push_block(LFH_arena *arena, LFH_block *block)
+{
+    block->flags = 0;
+    block->type = LFH_block_type_free;
+    block->next_free = arena->next_free;
+    arena->next_free = (UINT_PTR)block - (UINT_PTR)arena;
+    arena->used_count--;
+}
+
+static inline LFH_block *LFH_arena_pop_block(LFH_arena *arena)
+{
+    if (arena->next_free > 0)
+    {
+        LFH_block *block = LFH_arena_get_block(arena, arena->next_free);
+        arena->next_free = block->next_free;
+        arena->used_count++;
+        return block;
+    }
+    else
+    {
+        LFH_arena *child, *large_arena = LFH_large_arena_from_block((LFH_block *)arena);
+        LFH_class *class = LFH_class_from_arena(arena);
+        LFH_block *block = LFH_arena_get_block(arena, -arena->next_free);
+        ssize_t extra = 0, limit;
+
+        if (arena == large_arena)
+        {
+            extra = ARENA_HEADER_SIZE;
+            limit = LARGE_ARENA_SIZE;
+            child = LFH_large_arena_from_block(block);
+            if (arena != child) child->parent = arena;
+        }
+        else limit = LFH_class_from_arena(large_arena)->size;
+
+        arena->next_free -= class->size + extra;
+        if (-arena->next_free > limit - class->size)
+            arena->next_free = 0;
+
+        arena->used_count++;
+        return block;
+    }
+}
+
+static inline int LFH_arena_is_empty(LFH_arena *arena)
+{
+    return arena->next_free == 0;
+}
+
+static inline int LFH_arena_is_used(LFH_arena *arena)
+{
+    return arena->used_count > 0;
+}
+
+static inline int LFH_class_is_block(LFH_heap *heap, LFH_class *class)
+{
+    return class >= heap->block_class && class < (heap->block_class + TOTAL_BLOCK_CLASS_COUNT);
+}
+
+static void LFH_class_initialize(LFH_heap *heap, LFH_class *class, size_t index)
+{
+    class->next = NULL;
+
+    if (LFH_class_is_block(heap, class))
+    {
+        if (index <= SMALL_CLASS_LAST)
+            class->size = min(SMALL_CLASS_MIN_SIZE + SMALL_CLASS_STEP * (index - SMALL_CLASS_FIRST), SMALL_CLASS_MAX_SIZE);
+        else
+            class->size = min(MEDIUM_CLASS_MIN_SIZE + MEDIUM_CLASS_STEP * (index - MEDIUM_CLASS_FIRST), MEDIUM_CLASS_MAX_SIZE);
+    }
+    else
+    {
+        class->size = min(LARGE_CLASS_MIN_SIZE + LARGE_CLASS_STEP * (index - LARGE_CLASS_FIRST), LARGE_CLASS_MAX_SIZE);
+    }
+}
+
+static inline LFH_arena *LFH_class_pop_arena(LFH_class *class)
+{
+    LFH_arena *arena = class->next;
+    if (!arena) return NULL;
+    class->next = arena->class_entry;
+    return arena;
+}
+
+static inline void LFH_class_remove_arena(LFH_class *class, LFH_arena *arena)
+{
+    LFH_arena **next = &class->next;
+    while (*next != arena) next = &(*next)->class_entry;
+    *next = arena->class_entry;
+}
+
+static inline LFH_arena *LFH_class_peek_arena(LFH_class *class)
+{
+    return class->next;
+}
+
+static inline void LFH_class_push_arena(LFH_class *class, LFH_arena *arena)
+{
+    arena->class_entry = class->next;
+    class->next = arena;
+}
+
+static inline LFH_class *LFH_heap_get_class(LFH_heap *heap, size_t size)
+{
+    if (size == 0)
+        return &heap->block_class[0];
+    else if (size <= SMALL_CLASS_MAX_SIZE)
+        return &heap->block_class[SMALL_CLASS_FIRST + (size + SMALL_CLASS_MASK - SMALL_CLASS_MIN_SIZE) / SMALL_CLASS_STEP];
+    else if (size <= MEDIUM_CLASS_MAX_SIZE)
+        return &heap->block_class[MEDIUM_CLASS_FIRST + (size + MEDIUM_CLASS_MASK - MEDIUM_CLASS_MIN_SIZE) / MEDIUM_CLASS_STEP];
+    else if (size <= LARGE_CLASS_MAX_SIZE)
+        return &heap->large_class[LARGE_CLASS_FIRST + (size + LARGE_CLASS_MASK - LARGE_CLASS_MIN_SIZE) / LARGE_CLASS_STEP];
+    else
+        return NULL;
+}
+
+static void LFH_arena_initialize(LFH_heap *heap, LFH_class *class, LFH_arena *arena, size_t huge_size)
+{
+    arena->class = class;
+    arena->next_free = -ARENA_HEADER_SIZE;
+
+    if (class == NULL)
+        arena->huge_size = huge_size;
+    else
+        arena->used_count = 0;
+}
+
+static LFH_arena *LFH_acquire_arena(LFH_heap *heap, LFH_class *class);
+static BOOLEAN LFH_release_arena(LFH_heap *heap, LFH_arena *arena);
+
+static inline LFH_block *LFH_allocate_block(LFH_heap *heap, LFH_class *class, LFH_arena *arena);
+static inline BOOLEAN LFH_deallocate_block(LFH_heap *heap, LFH_arena *arena, LFH_block *block);
+
+static inline BOOLEAN LFH_deallocate_deferred_blocks(LFH_heap *heap)
+{
+    LFH_slist *entry = LFH_slist_flush(&heap->list_defer);
+
+    while (entry)
+    {
+        LFH_block *block = LIST_ENTRY(entry, LFH_block, entry_defer);
+        entry = entry->next;
+
+        if (!LFH_deallocate_block(heap, LFH_arena_from_block(block), block))
+            return FALSE;
+    }
+
+    return TRUE;
+}
+
+static inline void LFH_deallocated_cached_arenas(LFH_heap *heap)
+{
+    if (!heap->cached_large_arena) return;
+    LFH_memory_deallocate(heap->cached_large_arena, LARGE_ARENA_SIZE);
+    heap->cached_large_arena = NULL;
+}
+
+static inline size_t LFH_huge_alloc_size(size_t size)
+{
+    return (ARENA_HEADER_SIZE + size + BLOCK_ARENA_MASK) & ~BLOCK_ARENA_MASK;
+}
+
+static inline LFH_arena *LFH_allocate_huge_arena(LFH_heap *heap, size_t size)
+{
+    LFH_arena *arena;
+    size_t alloc_size = LFH_huge_alloc_size(size);
+    if (alloc_size < size) return NULL;
+
+    if ((arena = LFH_memory_allocate(alloc_size)))
+        LFH_arena_initialize(heap, NULL, arena, size);
+
+    return arena;
+}
+
+static inline LFH_arena *LFH_allocate_large_arena(LFH_heap *heap, LFH_class *class)
+{
+    LFH_arena *arena;
+
+    if ((arena = heap->cached_large_arena) ||
+        (arena = LFH_memory_allocate(LARGE_ARENA_SIZE)))
+    {
+        heap->cached_large_arena = NULL;
+        LFH_arena_initialize(heap, class, arena, 0);
+        LFH_class_push_arena(class, arena);
+    }
+
+    return arena;
+}
+
+static inline LFH_arena *LFH_allocate_block_arena(LFH_heap *heap, LFH_class *large_class, LFH_class *block_class)
+{
+    LFH_arena *large_arena;
+    LFH_arena *arena = NULL;
+
+    if ((large_arena = LFH_acquire_arena(heap, large_class)))
+    {
+        arena = (LFH_arena *)LFH_allocate_block(heap, large_class, large_arena);
+        LFH_arena_initialize(heap, block_class, arena, 0);
+        LFH_class_push_arena(block_class, arena);
+    }
+
+    return arena;
+}
+
+static inline LFH_arena *LFH_acquire_arena(LFH_heap *heap, LFH_class *class)
+{
+    LFH_arena *arena;
+
+    if (!(arena = LFH_class_peek_arena(class)))
+    {
+        if (LFH_class_is_block(heap, class))
+            arena = LFH_allocate_block_arena(heap, &heap->large_class[0], class);
+        else
+            arena = LFH_allocate_large_arena(heap, class);
+    }
+
+    return arena;
+}
+
+static inline BOOLEAN LFH_release_arena(LFH_heap *heap, LFH_arena *arena)
+{
+    LFH_arena *large_arena = LFH_large_arena_from_block((LFH_block *)arena);
+    if (arena == large_arena && !heap->cached_large_arena)
+    {
+        heap->cached_large_arena = arena;
+        return TRUE;
+    }
+    else if (arena == large_arena)
+        return LFH_memory_deallocate(arena, LARGE_ARENA_SIZE);
+    else
+        return LFH_deallocate_block(heap, large_arena, (LFH_block *)arena);
+};
+
+static inline LFH_block *LFH_allocate_block(LFH_heap *heap, LFH_class *class, LFH_arena *arena)
+{
+    LFH_block *block = LFH_arena_pop_block(arena);
+    if (LFH_arena_is_empty(arena))
+        LFH_class_pop_arena(class);
+    return block;
+}
+
+static inline BOOLEAN LFH_deallocate_block(LFH_heap *heap, LFH_arena *arena, LFH_block *block)
+{
+    LFH_class *class = LFH_class_from_arena(arena);
+
+    arena = LFH_parent_from_arena(arena);
+    if (LFH_arena_is_empty(arena))
+        LFH_class_push_arena(class, arena);
+
+    LFH_arena_push_block(arena, block);
+    if (LFH_arena_is_used(arena))
+        return TRUE;
+
+    LFH_class_remove_arena(class, arena);
+    return LFH_release_arena(heap, arena);
+}
+
+static void LFH_heap_initialize(LFH_heap *heap)
+{
+    size_t i;
+
+    for (i = 0; i < TOTAL_LARGE_CLASS_COUNT; ++i)
+        LFH_class_initialize(heap, &heap->large_class[i], i);
+    for (i = 0; i < TOTAL_BLOCK_CLASS_COUNT; ++i)
+        LFH_class_initialize(heap, &heap->block_class[i], i);
+
+    heap->list_defer = NULL;
+    heap->cached_large_arena = NULL;
+}
+
+static SLIST_HEADER *LFH_orphan_list(void)
+{
+    static SLIST_HEADER *header;
+    SLIST_HEADER *ptr, *expected = NULL;
+    LFH_heap *tmp;
+
+    C_ASSERT(sizeof(LFH_heap) >= sizeof(SLIST_HEADER));
+
+    if ((ptr = __atomic_load_n(&header, __ATOMIC_RELAXED)))
+        return ptr;
+
+    if (!(ptr = LFH_memory_allocate(BLOCK_ARENA_SIZE)))
+        return NULL;
+
+    RtlInitializeSListHead(ptr);
+    for (tmp = (LFH_heap *)ptr + 1; tmp < (LFH_heap *)ptr + BLOCK_ARENA_SIZE / sizeof(*tmp); tmp++)
+    {
+        LFH_heap_initialize(tmp);
+        RtlInterlockedPushEntrySList(ptr, &tmp->entry_orphan);
+    }
+
+    if (__atomic_compare_exchange_n(&header, &expected, ptr, 0, __ATOMIC_RELEASE, __ATOMIC_ACQUIRE))
+        return ptr;
+
+    LFH_memory_deallocate(ptr, BLOCK_ARENA_SIZE);
+    return expected;
+}
+
+static void LFH_heap_finalize(LFH_heap *heap)
+{
+    LFH_arena *arena;
+
+    LFH_deallocate_deferred_blocks(heap);
+
+    for (size_t i = 0; i < TOTAL_BLOCK_CLASS_COUNT; ++i)
+    {
+        while ((arena = LFH_class_pop_arena(&heap->block_class[i])))
+        {
+            WARN("block arena %p still has used blocks\n", arena);
+            LFH_release_arena(heap, arena);
+        }
+    }
+
+    for (size_t i = 0; i < TOTAL_LARGE_CLASS_COUNT; ++i)
+    {
+        while ((arena = LFH_class_pop_arena(&heap->large_class[i])))
+        {
+            WARN("large arena %p still has used blocks\n", arena);
+            LFH_memory_deallocate(arena, LARGE_ARENA_SIZE);
+        }
+    }
+
+    LFH_deallocated_cached_arenas(heap);
+}
+
+static LFH_heap *LFH_heap_allocate(void)
+{
+    SLIST_HEADER *list_orphan = LFH_orphan_list();
+    LFH_heap *heap, *tmp;
+
+    heap = LFH_memory_allocate(BLOCK_ARENA_SIZE);
+    if (!heap)
+        return NULL;
+
+    for (tmp = heap + 1; tmp < heap + BLOCK_ARENA_SIZE / sizeof(*tmp); tmp++)
+    {
+        LFH_heap_initialize(tmp);
+        RtlInterlockedPushEntrySList(list_orphan, &tmp->entry_orphan);
+    }
+
+    LFH_heap_initialize(heap);
+    return heap;
+}
+
+static LFH_heap *LFH_create_thread_heap(void)
+{
+    SLIST_ENTRY *entry;
+    LFH_heap *heap;
+
+    if ((entry = RtlInterlockedPopEntrySList(LFH_orphan_list())))
+        heap = LIST_ENTRY(entry, LFH_heap, entry_orphan);
+    else
+        heap = LFH_heap_allocate();
+
+    return (NtCurrentTeb()->Reserved5[2] = heap);
+}
+
+static inline LFH_heap *LFH_thread_heap(BOOL create)
+{
+    LFH_heap *heap = (LFH_heap *)NtCurrentTeb()->Reserved5[2];
+    if (!heap && create) return LFH_create_thread_heap();
+    return heap;
+}
+
+static void LFH_dump_arena(LFH_heap *heap, LFH_class *class, LFH_arena *arena)
+{
+    LFH_arena *large_arena = LFH_large_arena_from_block((LFH_block *)arena);
+    LFH_arena *block_arena = LFH_block_arena_from_block((LFH_block *)arena);
+
+    if (arena == block_arena)
+        WARN("    block arena: %p-%p", arena, (void *)((UINT_PTR)large_arena + BLOCK_ARENA_SIZE - 1));
+    else if (arena == large_arena)
+        WARN("    large arena: %p-%p", arena, (void *)((UINT_PTR)large_arena + LARGE_ARENA_SIZE - 1));
+
+    WARN(" heap: %p class: %p parent: %p free: %Id used: %Id\n",
+          LFH_heap_from_arena(arena), LFH_class_from_arena(arena), LFH_parent_from_arena(arena), arena->next_free, arena->used_count);
+}
+
+static void LFH_dump_class(LFH_heap *heap, LFH_class *class)
+{
+    LFH_arena *arena = class->next;
+    if (!arena) return;
+
+    if (LFH_class_is_block(heap, class))
+        WARN("  block class: %p size: %Ix\n", class, class->size);
+    else
+        WARN("  large class: %p size: %Ix\n", class, class->size);
+
+    while (arena)
+    {
+        LFH_dump_arena(heap, class, arena);
+        arena = arena->class_entry;
+    }
+}
+
+static void LFH_dump_heap(LFH_heap *heap)
+{
+    size_t i;
+
+    WARN("heap: %p\n", heap);
+
+    for (i = 0; i < TOTAL_BLOCK_CLASS_COUNT; ++i)
+        LFH_dump_class(heap, &heap->block_class[i]);
+
+    for (i = 0; i < TOTAL_LARGE_CLASS_COUNT; ++i)
+        LFH_dump_class(heap, &heap->large_class[i]);
+}
+
+static BOOLEAN LFH_validate_arena(ULONG flags, const LFH_arena *arena);
+static BOOLEAN LFH_validate_heap(ULONG flags, const LFH_heap *heap);
+
+static inline BOOLEAN LFH_validate_block(ULONG flags, const LFH_block *block)
+{
+    const LFH_arena *arena = LFH_arena_from_block(block);
+    const LFH_arena *large_arena = LFH_large_arena_from_block(block);
+    const LFH_arena *block_arena = LFH_block_arena_from_block(block);
+    const LFH_arena *arena_arena = LFH_large_arena_from_block((LFH_block *)arena);
+    const char *err = NULL;
+
+    if (unlikely(flags & HEAP_VALIDATE))
+        return LFH_validate_arena(flags, arena);
+
+    if (unlikely(!arena))
+        err = "invalid arena";
+    else if (unlikely(arena != arena_arena && arena != (arena_arena + 1)))
+        err = "invalid arena alignment";
+    else if (likely(arena == block_arena))
+    {
+        if (unlikely((UINT_PTR)block < (UINT_PTR)block_arena + ARENA_HEADER_SIZE))
+            err = "invalid block alignment";
+        if (unlikely(((UINT_PTR)block & (sizeof(*block) - 1))))
+            err = "invalid block alignment";
+    }
+    else if (unlikely(arena != large_arena))
+        err = "large/huge arena mismatch";
+    else if (unlikely((UINT_PTR)block != (UINT_PTR)block_arena))
+        err = "invalid block for large/huge arena";
+
+    if (unlikely(err)) WARN("%08lx %p: %s\n", flags, block, err);
+    return err == NULL;
+}
+
+static BOOLEAN LFH_validate_free_block(ULONG flags, const LFH_block *block)
+{
+    const char *err = NULL;
+
+    if (unlikely(!LFH_validate_block(flags, block)))
+        return FALSE;
+    if (unlikely(block->type != LFH_block_type_free))
+        err = "invalid free block type";
+
+    if (unlikely(err)) WARN("%08lx %p: %s\n", flags, block, err);
+    return err == NULL;
+}
+
+static BOOLEAN LFH_validate_defer_block(ULONG flags, const LFH_block *block)
+{
+    const char *err = NULL;
+
+    if (unlikely(!LFH_validate_block(flags, block)))
+        return FALSE;
+    if (unlikely(block->type != LFH_block_type_free))
+        err = "invalid defer block type";
+    else if (unlikely(flags & HEAP_FREE_CHECKING_ENABLED))
+    {
+        const unsigned int *data = (const unsigned int *)LFH_ptr_from_block(block);
+        size_t class_size = LFH_block_get_class_size(block);
+        for (size_t i = 0; i < class_size / 4 - (data - (const unsigned int *)block) && !err; ++i)
+            if (unlikely(data[i] != 0xfeeefeee)) err = "invalid free filler";
+    }
+
+    if (unlikely(err)) WARN("%08lx %p: %s\n", flags, block, err);
+    return err == NULL;
+}
+
+static inline BOOLEAN LFH_validate_used_block(ULONG flags, const LFH_block *block)
+{
+    const char *err = NULL;
+
+    if (unlikely(!LFH_validate_block(flags, block)))
+        return FALSE;
+    if (unlikely(block->type != LFH_block_type_used))
+        err = "invalid used block type";
+    else if (unlikely(flags & HEAP_TAIL_CHECKING_ENABLED))
+    {
+        const unsigned char *data = (const unsigned char *)LFH_ptr_from_block(block);
+        size_t alloc_size = LFH_block_get_alloc_size(block, flags);
+        for (size_t i = alloc_size; i < alloc_size + ALIGNMENT && !err; ++i)
+            if (data[i] != 0xab) err = "invalid tail filler";
+    }
+
+    if (unlikely(err)) WARN("%08lx %p: %s\n", flags, block, err);
+    return err == NULL;
+}
+
+static BOOLEAN LFH_validate_arena_free_blocks(ULONG flags, const LFH_arena *arena)
+{
+    ssize_t offset = arena->next_free;
+    while (offset > 0)
+    {
+        LFH_block *block = LFH_arena_get_block(arena, offset);
+        if (!LFH_validate_free_block(flags, block))
+            return FALSE;
+        offset = block->next_free;
+    }
+
+    return TRUE;
+}
+
+static BOOLEAN LFH_validate_arena(ULONG flags, const LFH_arena *arena)
+{
+    const char *err = NULL;
+    const LFH_arena *parent;
+    const LFH_arena *block_arena = LFH_block_arena_from_block((LFH_block *)arena);
+    const LFH_arena *large_arena = LFH_large_arena_from_block((LFH_block *)arena);
+
+    if (unlikely(flags & HEAP_VALIDATE))
+        return LFH_validate_heap(flags, LFH_heap_from_arena(arena));
+
+    if (unlikely(arena != large_arena && arena != block_arena))
+        err = "invalid arena alignment";
+    else if (unlikely(arena == block_arena))
+    {
+        if (unlikely(!LFH_validate_block(flags, (LFH_block *)arena)))
+            err = "invalid block arena";
+        else if (unlikely(!LFH_validate_arena_free_blocks(flags, arena)))
+            err = "invalid block arena free list";
+    }
+    else if (unlikely(arena == large_arena && !LFH_class_from_arena(arena)))
+    {
+        if (unlikely(arena->huge_size <= LARGE_CLASS_MAX_SIZE))
+            err = "invalid huge arena size";
+    }
+    else if (unlikely(arena == large_arena && (parent = LFH_parent_from_arena(arena)) != arena))
+    {
+        if (unlikely(arena > parent || LFH_large_arena_from_block((LFH_block *)parent) != parent))
+            err = "invalid child arena parent";
+    }
+    else
+    {
+        if (unlikely(!LFH_validate_arena_free_blocks(flags, arena)))
+            err = "invalid large arena free list";
+    }
+
+    if (unlikely(err)) WARN("%08lx %p: %s\n", flags, arena, err);
+    return err == NULL;
+}
+
+static BOOLEAN LFH_validate_class_arenas(ULONG flags, const LFH_class *class)
+{
+    LFH_arena *arena = class->next;
+    while (arena)
+    {
+        if (!LFH_validate_arena(flags, arena))
+            return FALSE;
+
+        arena = arena->class_entry;
+    }
+
+    return TRUE;
+}
+
+static BOOLEAN LFH_validate_heap_defer_blocks(ULONG flags, const LFH_heap *heap)
+{
+    const LFH_slist *entry = heap->list_defer;
+
+    while (entry)
+    {
+        const LFH_block *block = LIST_ENTRY(entry, LFH_block, entry_defer);
+        if (!LFH_validate_defer_block(flags, block))
+            return FALSE;
+        entry = entry->next;
+    }
+
+    return TRUE;
+}
+
+static BOOLEAN LFH_validate_heap(ULONG flags, const LFH_heap *heap)
+{
+    const char *err = NULL;
+    UINT i;
+
+    flags &= ~HEAP_VALIDATE;
+
+    if (heap != LFH_thread_heap(FALSE))
+        err = "unable to validate foreign heap";
+    else if (!LFH_validate_heap_defer_blocks(flags, heap))
+        err = "invalid heap defer blocks";
+    else
+    {
+        for (i = 0; err == NULL && i < TOTAL_BLOCK_CLASS_COUNT; ++i)
+        {
+            if (!LFH_validate_class_arenas(flags, &heap->block_class[i]))
+                return FALSE;
+        }
+
+        for (i = 0; err == NULL && i < TOTAL_LARGE_CLASS_COUNT; ++i)
+        {
+            if (!LFH_validate_class_arenas(flags, &heap->large_class[i]))
+                return FALSE;
+        }
+    }
+
+    if (unlikely(err)) WARN("%08lx %p: %s\n", flags, heap, err);
+    return err == NULL;
+}
+
+static inline void LFH_block_initialize(LFH_block *block, ULONG flags, size_t old_size, size_t new_size, size_t class_size)
+{
+    char *ptr = (char *)LFH_ptr_from_block(block);
+
+    TRACE("block %p, flags %08lx, old_size %Ix, new_size %Ix, class_size %Ix, ptr %p\n", block, flags, old_size, new_size, class_size, ptr);
+
+    if ((flags & HEAP_ZERO_MEMORY) && new_size > old_size)
+        memset(ptr + old_size, 0, new_size - old_size);
+    else if ((flags & HEAP_FREE_CHECKING_ENABLED) && new_size > old_size && class_size < BLOCK_ARENA_SIZE)
+        memset(ptr + old_size, 0x55, new_size - old_size);
+
+    if ((flags & HEAP_TAIL_CHECKING_ENABLED))
+        memset(ptr + new_size, 0xab, class_size - new_size - (ptr - (char *)block));
+
+    block->flags = BLOCK_USER_FLAGS( flags );
+    block->type = LFH_block_type_used;
+    block->alloc_size = new_size;
+}
+
+static FORCEINLINE LFH_ptr *LFH_allocate(ULONG flags, size_t size)
+{
+    LFH_block *block = NULL;
+    LFH_class *class;
+    LFH_arena *arena;
+    LFH_heap *heap = LFH_thread_heap(TRUE);
+    size_t class_size = LFH_get_class_size(flags, size);
+
+    if (class_size == ~(size_t)0)
+        return NULL;
+
+    if (!LFH_deallocate_deferred_blocks(heap))
+        return NULL;
+
+    if ((class = LFH_heap_get_class(heap, class_size)))
+    {
+        arena = LFH_acquire_arena(heap, class);
+        if (arena) block = LFH_allocate_block(heap, class, arena);
+        if (block) LFH_block_initialize(block, flags, 0, size, LFH_block_get_class_size(block));
+    }
+    else
+    {
+        arena = LFH_allocate_huge_arena(heap, class_size);
+        if (arena) block = LFH_arena_get_block(arena, ARENA_HEADER_SIZE);
+        if (block) LFH_block_initialize(block, flags & ~HEAP_ZERO_MEMORY, 0, size, LFH_block_get_class_size(block));
+    }
+
+    LFH_deallocated_cached_arenas(heap);
+
+    if (!block) return NULL;
+    return LFH_ptr_from_block(block);
+}
+
+static FORCEINLINE BOOLEAN LFH_free(ULONG flags, LFH_ptr *ptr)
+{
+    LFH_block *block = LFH_block_from_ptr(ptr);
+    LFH_arena *arena = LFH_arena_from_block(block);
+    LFH_heap *heap = LFH_heap_from_arena(arena);
+
+    if (!LFH_class_from_arena(arena))
+        return LFH_memory_deallocate(arena, LFH_block_get_class_size(block));
+
+    if (flags & HEAP_FREE_CHECKING_ENABLED)
+    {
+        unsigned int *data = (unsigned int *)LFH_ptr_from_block(block);
+        size_t class_size = LFH_block_get_class_size(block);
+        for (size_t i = 0; i < class_size / 4 - (data - (const unsigned int *)block); ++i)
+            data[i] = 0xfeeefeee;
+    }
+
+    block->flags = 0;
+    block->type = LFH_block_type_free;
+
+    if (heap == LFH_thread_heap(FALSE) && !(flags & HEAP_FREE_CHECKING_ENABLED))
+        LFH_deallocate_block(heap, LFH_arena_from_block(block), block);
+    else
+        LFH_slist_push(&heap->list_defer, &block->entry_defer);
+
+    return TRUE;
+}
+
+static FORCEINLINE LFH_ptr *LFH_reallocate(ULONG flags, LFH_ptr *old_ptr, size_t new_size)
+{
+    LFH_block *block = LFH_block_from_ptr(old_ptr);
+    LFH_arena *arena = LFH_arena_from_block(block);
+    LFH_heap *heap = LFH_heap_from_arena(arena);
+    size_t old_size = LFH_block_get_alloc_size(block, flags);
+    size_t old_class_size = LFH_block_get_class_size(block);
+    size_t new_class_size = LFH_get_class_size(flags, new_size);
+    LFH_class *new_class, *old_class = LFH_class_from_arena(arena);
+    LFH_ptr *new_ptr = NULL;
+
+    if (new_class_size == ~(size_t)0)
+        return NULL;
+
+    if (new_class_size <= old_class_size)
+        goto in_place;
+
+    if ((new_class = LFH_heap_get_class(heap, new_class_size)) && new_class == old_class)
+        goto in_place;
+
+    old_class_size = LFH_huge_alloc_size(old_class_size);
+    new_class_size = LFH_huge_alloc_size(new_class_size);
+    if (!new_class && !old_class && old_class_size == new_class_size)
+        goto in_place;
+
+    if (flags & HEAP_REALLOC_IN_PLACE_ONLY)
+        return NULL;
+
+    if (!(new_ptr = LFH_allocate(flags, new_size)))
+        return NULL;
+
+    memcpy(new_ptr, old_ptr, old_size);
+
+    if (LFH_free(flags, old_ptr))
+        return new_ptr;
+
+    LFH_free(flags, new_ptr);
+    return NULL;
+
+in_place:
+    LFH_block_initialize(block, flags, old_size, new_size, old_class_size);
+    return old_ptr;
+}
+
+static inline size_t LFH_get_allocated_size(ULONG flags, const LFH_ptr *ptr)
+{
+    const LFH_block *block = LFH_block_from_ptr(ptr);
+    return LFH_block_get_alloc_size(block, flags);
+}
+
+static inline BOOLEAN LFH_validate(ULONG flags, const LFH_ptr *ptr)
+{
+    const LFH_block *block = LFH_block_from_ptr(ptr);
+    const LFH_heap *heap;
+
+    /* clear HEAP_VALIDATE so we only validate block */
+    if (likely(ptr))
+        return LFH_validate_used_block(flags & ~HEAP_VALIDATE, block);
+
+    if (!(heap = LFH_thread_heap(FALSE)))
+        return TRUE;
+
+    return LFH_validate_heap(flags, heap);
+}
+
+static inline void LFH_get_user_info( ULONG flags, void *ptr, void **user_value, ULONG *user_flags )
+{
+    LFH_block *block = LFH_block_from_ptr(ptr);
+    void **tmp;
+
+    if (!(*user_flags = HEAP_USER_FLAGS( block->flags ))) return;
+
+    tmp = (void **)((char *)block + LFH_block_get_class_size( block ) - ALIGNMENT);
+    *user_flags = *user_flags & ~HEAP_ADD_USER_INFO;
+    *user_value = tmp[1];
+}
+
+static inline void LFH_set_user_value( ULONG flags, void *ptr, void *user_value )
+{
+    LFH_block *block = LFH_block_from_ptr(ptr);
+    void **tmp;
+
+    if (!(block->flags & BLOCK_FLAG_USER_INFO)) return;
+
+    tmp = (void **)((char *)block + LFH_block_get_class_size( block ) - ALIGNMENT);
+    tmp[1] = user_value;
+}
+
+static inline void LFH_set_user_flags( ULONG flags, void *ptr, ULONG clear, ULONG set )
+{
+    LFH_block *block = LFH_block_from_ptr(ptr);
+
+    if (!(block->flags & BLOCK_FLAG_USER_INFO)) return;
+
+    block->flags &= ~BLOCK_USER_FLAGS( clear );
+    block->flags |= BLOCK_USER_FLAGS( set );
+}
+
+static inline BOOLEAN LFH_try_validate_all(ULONG flags)
+{
+    if (likely(!(flags & HEAP_VALIDATE_ALL)))
+        return TRUE;
+
+    if (likely(LFH_validate(flags, NULL)))
+        return TRUE;
+
+    LFH_dump_heap(LFH_thread_heap(FALSE));
+    return FALSE;
+}
+
+NTSTATUS HEAP_lfh_allocate(HANDLE heap, ULONG flags, SIZE_T size, void **out)
+{
+    TRACE("heap %p, flags %08lx, size %Ix, out %p.\n", heap, flags, size, out);
+
+    if (unlikely(!LFH_try_validate_all(flags)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!(*out = LFH_allocate(flags, size))))
+        return STATUS_NO_MEMORY;
+
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_free(HANDLE heap, ULONG flags, void *ptr)
+{
+    TRACE("heap %p, flags %08lx, ptr %p.\n", heap, flags, ptr);
+
+    if (unlikely(!LFH_try_validate_all(flags)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!LFH_validate(flags, ptr)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!LFH_free(flags, ptr)))
+        return STATUS_INVALID_PARAMETER;
+
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_reallocate(HANDLE heap, ULONG flags, void *ptr, SIZE_T size, void **out)
+{
+    TRACE("heap %p, flags %08lx, ptr %p, size %Ix, out %p.\n", heap, flags, ptr, size, out);
+
+    if (unlikely(!LFH_try_validate_all(flags)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!LFH_validate(flags, ptr)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!(*out = LFH_reallocate(flags, ptr, size))))
+        return STATUS_NO_MEMORY;
+
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_get_allocated_size(HANDLE heap, ULONG flags, const void *ptr, SIZE_T* out)
+{
+    TRACE("heap %p, flags %08lx, ptr %p, out %p.\n", heap, flags, ptr, out);
+
+    if (unlikely(!LFH_try_validate_all(flags)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!LFH_validate(flags, ptr)))
+        return STATUS_INVALID_PARAMETER;
+
+    *out = LFH_get_allocated_size(flags, ptr);
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_validate(HANDLE heap, ULONG flags, const void *ptr)
+{
+    TRACE("heap %p, flags %08lx, ptr %p.\n", heap, flags, ptr);
+
+    if (unlikely(!LFH_try_validate_all(flags)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!LFH_validate(flags, ptr)))
+        return STATUS_INVALID_PARAMETER;
+
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_get_user_info( HANDLE heap, ULONG flags, void *ptr, void **user_value, ULONG *user_flags )
+{
+    TRACE("heap %p, flags %08lx, ptr %p, user_value %p, user_flags %p.\n", heap, flags, ptr, user_value, user_flags);
+
+    if (unlikely(!LFH_try_validate_all(flags)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!LFH_validate(flags, ptr)))
+        return STATUS_INVALID_PARAMETER;
+
+    LFH_get_user_info(flags, ptr, user_value, user_flags);
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_set_user_value( HANDLE heap, ULONG flags, void *ptr, void *user_value )
+{
+    TRACE("heap %p, flags %08lx, ptr %p, user_value %p.\n", heap, flags, ptr, user_value);
+
+    if (unlikely(!LFH_try_validate_all(flags)))
+        return STATUS_INVALID_PARAMETER;
+
+    if (unlikely(!LFH_validate(flags, ptr)))
+        return STATUS_INVALID_PARAMETER;
+
+    LFH_set_user_value(flags, ptr, user_value);
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_set_user_flags( HANDLE heap, ULONG flags, void *ptr, ULONG clear, ULONG set )
+{
+    TRACE("heap %p, flags %08lx, ptr %p, clear %08lx, set %08lx.\n", heap, flags, ptr, clear, set);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_validate(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    LFH_set_user_flags(flags, ptr, clear, set);
+    return STATUS_SUCCESS;
+}
+
+void HEAP_lfh_notify_thread_destroy(BOOLEAN last)
+{
+    SLIST_HEADER *list_orphan = LFH_orphan_list();
+    SLIST_ENTRY *entry_orphan = NULL;
+    LFH_heap *heap;
+
+    if (last)
+    {
+        while ((entry_orphan || (entry_orphan = RtlInterlockedFlushSList(list_orphan))))
+        {
+            LFH_heap *orphan = LIST_ENTRY(entry_orphan, LFH_heap, entry_orphan);
+            entry_orphan = entry_orphan->Next;
+            LFH_heap_finalize(orphan);
+        }
+        LFH_memory_deallocate(list_orphan, BLOCK_ARENA_SIZE);
+    }
+    else if ((heap = LFH_thread_heap(FALSE)) && LFH_validate_heap(0, heap))
+        RtlInterlockedPushEntrySList(list_orphan, &heap->entry_orphan);
+}
+
+void HEAP_lfh_set_debug_flags(ULONG flags)
+{
+    LFH_heap *heap = LFH_thread_heap(FALSE);
+    if (!heap) return;
+
+    LFH_deallocate_deferred_blocks(heap);
+    LFH_deallocated_cached_arenas(heap);
+}
diff --git a/dlls/ntdll/loader.c b/dlls/ntdll/loader.c
index c97eb04188b..90fb81b7cc4 100644
--- a/dlls/ntdll/loader.c
+++ b/dlls/ntdll/loader.c
@@ -3808,6 +3808,7 @@ void WINAPI RtlExitUserProcess( DWORD status )
     RtlAcquirePebLock();
     NtTerminateProcess( 0, status );
     LdrShutdownProcess();
+    HEAP_notify_thread_destroy(TRUE);
     for (;;) NtTerminateProcess( GetCurrentProcess(), status );
 }
 
@@ -4251,6 +4252,7 @@ void WINAPI LdrInitializeThunk( CONTEXT *context, ULONG_PTR unknown2, ULONG_PTR
         ANSI_STRING func_name;
         WINE_MODREF *kernel32;
         PEB *peb = NtCurrentTeb()->Peb;
+        DWORD hci = 2;
 
         NtQueryVirtualMemory( GetCurrentProcess(), LdrInitializeThunk, MemoryBasicInformation,
                               &meminfo, sizeof(meminfo), NULL );
@@ -4283,6 +4285,7 @@ void WINAPI LdrInitializeThunk( CONTEXT *context, ULONG_PTR unknown2, ULONG_PTR
         wm->ldr.LoadCount = -1;
 
         build_ntdll_module( meminfo.AllocationBase );
+        RtlSetHeapInformation( GetProcessHeap(), HeapCompatibilityInformation, &hci, sizeof(hci) );
 
         if (NtCurrentTeb()->WowTebOffset) init_wow64( context );
 
diff --git a/dlls/ntdll/ntdll_misc.h b/dlls/ntdll/ntdll_misc.h
index 78ff79625ea..92c26ae62fc 100644
--- a/dlls/ntdll/ntdll_misc.h
+++ b/dlls/ntdll/ntdll_misc.h
@@ -147,6 +147,33 @@ static inline TEB64 *NtCurrentTeb64(void) { return NULL; }
 static inline TEB64 *NtCurrentTeb64(void) { return (TEB64 *)NtCurrentTeb()->GdiBatchCount; }
 #endif
 
+#define HEAP_STD 0
+#define HEAP_LAL 1
+#define HEAP_LFH 2
+
+/* some undocumented flags (names are made up) */
+#define HEAP_PRIVATE          0x00001000
+#define HEAP_ADD_USER_INFO    0x00000100
+#define HEAP_USER_FLAGS_MASK  0x00000f00
+#define HEAP_PAGE_ALLOCS      0x01000000
+#define HEAP_VALIDATE         0x10000000
+#define HEAP_VALIDATE_ALL     0x20000000
+#define HEAP_VALIDATE_PARAMS  0x40000000
+#define HEAP_CHECKING_ENABLED 0x80000000
+
+NTSTATUS HEAP_lfh_allocate( HANDLE std_heap, ULONG flags, SIZE_T size, void **out );
+NTSTATUS HEAP_lfh_free( HANDLE std_heap, ULONG flags, void *ptr );
+NTSTATUS HEAP_lfh_reallocate( HANDLE std_heap, ULONG flags, void *ptr, SIZE_T size, void **out );
+NTSTATUS HEAP_lfh_get_allocated_size( HANDLE std_heap, ULONG flags, const void *ptr, SIZE_T *out );
+NTSTATUS HEAP_lfh_validate( HANDLE std_heap, ULONG flags, const void *ptr );
+NTSTATUS HEAP_lfh_get_user_info( HANDLE std_handle, ULONG flags, void *ptr, void **user_value, ULONG *user_flags );
+NTSTATUS HEAP_lfh_set_user_value( HANDLE std_handle, ULONG flags, void *ptr, void *user_value );
+NTSTATUS HEAP_lfh_set_user_flags( HANDLE std_handle, ULONG flags, void *ptr, ULONG clear, ULONG set );
+
+void HEAP_notify_thread_destroy( BOOLEAN last );
+void HEAP_lfh_notify_thread_destroy( BOOLEAN last );
+void HEAP_lfh_set_debug_flags( ULONG flags );
+
 #define HASH_STRING_ALGORITHM_DEFAULT  0
 #define HASH_STRING_ALGORITHM_X65599   1
 #define HASH_STRING_ALGORITHM_INVALID  0xffffffff
diff --git a/dlls/ntdll/thread.c b/dlls/ntdll/thread.c
index 1bd8f900d22..6a3e47839c4 100644
--- a/dlls/ntdll/thread.c
+++ b/dlls/ntdll/thread.c
@@ -233,6 +233,7 @@ void WINAPI RtlExitUserThread( ULONG status )
     NtQueryInformationThread( GetCurrentThread(), ThreadAmILastThread, &last, sizeof(last), NULL );
     if (last) RtlExitUserProcess( status );
     LdrShutdownThread();
+    HEAP_notify_thread_destroy(FALSE);
     for (;;) NtTerminateThread( GetCurrentThread(), status );
 }
 
diff --git a/dlls/ntdll/unix/unix_private.h b/dlls/ntdll/unix/unix_private.h
index 25c2f93bb40..17d144e5321 100644
--- a/dlls/ntdll/unix/unix_private.h
+++ b/dlls/ntdll/unix/unix_private.h
@@ -67,6 +67,7 @@ struct ntdll_thread_data
     PRTL_THREAD_START_ROUTINE start;  /* thread entry point */
     void              *param;         /* thread entry point parameter */
     void              *jmp_buf;       /* setjmp buffer for exception handling */
+    void              *heap;          /* thread local heap data */
 };
 
 C_ASSERT( sizeof(struct ntdll_thread_data) <= sizeof(((TEB *)0)->GdiTebBatch) );
